{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01_through_the_cycle_pd — Skeleton Notebook (TTC PD)\n",
        "\n",
        "**Purpose:** A structured, reproducible skeleton for Through-the-Cycle Probability of Default model development. This notebook follows a disciplined flow: Data strategy → EDA → Preparation & feature engineering → Model design → Calibration → Validation → Documentation & monitoring.\n",
        "\n",
        "This skeleton is heavily commented so you (or a reviewer) can follow reasoning, insert real data paths, and extend each section into production-ready components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Quick instructions\n",
        "- Edit `DATA_PATH` in the first code cell to point at your raw CSV (example: `data/raw/GiveMeSomeCredit/cs-training.csv`).\n",
        "- Run cells top-to-bottom. Each code cell is commented with **what**, **why**, and **alternatives**.\n",
        "- For TTC workflows you will eventually add macro series and perform cyclical adjustment (placeholders included).\n",
        "- Save frequently and keep artifacts (models, preprocessor) under `artifacts/`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Setup & imports\n",
        "\n",
        "This cell installs (optionally) and imports required packages. In Colab you may uncomment installs. Use virtualenv/conda locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Optional installs for Colab — uncomment if needed\n",
        "# !pip install -q scikit-learn pandas matplotlib seaborn joblib\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
        "import joblib\n",
        "\n",
        "print('Setup complete. Edit DATA_PATH below to point to your csv file and run the next cell.')\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Data path & robust load\n",
        "\n",
        "Set `DATA_PATH` to the CSV file. We try a couple of common locations to be convenient. If file not found, change the path.\n",
        "\n",
        "Why: make the notebook portable between local and Colab runs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === EDIT THIS PATH if necessary ===\n",
        "candidates = [\n",
        "    'data/raw/GiveMeSomeCredit/cs-training.csv',\n",
        "    'data/GiveMeSomeCredit/cs-training.csv',\n",
        "    'data/sample_pit.csv'\n",
        "]\n",
        "\n",
        "DATA_PATH = None\n",
        "for p in candidates:\n",
        "    if Path(p).exists():\n",
        "        DATA_PATH = p\n",
        "        break\n",
        "\n",
        "if DATA_PATH is None:\n",
        "    raise FileNotFoundError('Set DATA_PATH to your dataset location. Tried: ' + ', '.join(candidates))\n",
        "\n",
        "print('Loading from:', DATA_PATH)\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print('Loaded shape:', df.shape)\n",
        "display(df.head())\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Data Strategy & Governance (notes + quick checks)\n",
        "\n",
        "Before heavy EDA, document: portfolio scope, default definition, observation/performance windows, and any exclusions (e.g., cosigned loans). Below we run quick checks for these items and for data quality.\n",
        "\n",
        "Add a text cell or external MDD for full governance documentation in production."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick governance checks (editable)\n",
        "print('Number of rows:', len(df))\n",
        "print('Columns:', df.columns.tolist())\n",
        "print('\\nDtypes:')\n",
        "display(df.dtypes)\n",
        "\n",
        "# Identify target candidate column (dataset-specific). Update TARGET if necessary.\n",
        "if 'SeriousDlqin2yrs' in df.columns:\n",
        "    TARGET = 'SeriousDlqin2yrs'\n",
        "else:\n",
        "    # fallback: consider last column as target\n",
        "    TARGET = df.columns[-1]\n",
        "\n",
        "print('\\nAssumed TARGET =', TARGET)\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Exploratory Data Analysis (comprehensive)\n",
        "\n",
        "We'll inspect:\n",
        "- Missingness and patterns\n",
        "- Summary statistics and distributions\n",
        "- Target-wise feature differences\n",
        "- Correlations and multicollinearity hints\n",
        "- Simple bivariate visuals\n",
        "\n",
        "Be mindful of leakage: ensure features do not directly include future information (e.g., post-default actions)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 4.1 Missingness\n",
        "miss = df.isnull().sum().sort_values(ascending=False)\n",
        "miss = miss[miss>0]\n",
        "if len(miss) > 0:\n",
        "    print('Columns with missing values:')\n",
        "    display(miss)\n",
        "else:\n",
        "    print('No missing values detected')\n",
        "\n",
        "# 4.2 Basic numeric summary\n",
        "display(df.describe(include='all').T)\n",
        "\n",
        "# 4.3 Target distribution\n",
        "print('\\nTarget distribution:')\n",
        "display(df[TARGET].value_counts(dropna=False))\n",
        "\n",
        "# 4.4 Univariate numeric plots (first N numeric cols)\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "num_cols = [c for c in num_cols if c != TARGET]\n",
        "plot_cols = num_cols[:8]  # limit visuals to a manageable number in skeleton\n",
        "for c in plot_cols:\n",
        "    plt.figure(figsize=(6,2.5))\n",
        "    sns.histplot(df[c].dropna(), bins=40)\n",
        "    plt.title(f'Distribution of {c}')\n",
        "    plt.show()\n",
        "\n",
        "# 4.5 Target-wise boxplots for numeric features (illustrative)\n",
        "for c in plot_cols:\n",
        "    plt.figure(figsize=(6,2.5))\n",
        "    sns.boxplot(x=TARGET, y=c, data=df)\n",
        "    plt.title(f'{c} by target')\n",
        "    plt.show()\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Data transformation & feature engineering (skeleton)\n",
        "\n",
        "Tasks here:\n",
        "- Define features to keep / drop\n",
        "- Imputation rules (numeric -> median; categorical -> mode)\n",
        "- Encoding (WoE or OneHot for small cardinality)\n",
        "- Scaling for linear models\n",
        "- Placeholder for cyclical adjustment (regress feature on macro index and keep residual)\n",
        "\n",
        "Each choice should be documented in MDD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === Simple feature selection heuristic ===\n",
        "all_features = [c for c in df.columns if c != TARGET]\n",
        "\n",
        "# Demonstration: drop ID-like cols if present\n",
        "drop_cols = [c for c in all_features if 'id' in c.lower() or c.lower().endswith('id')]\n",
        "features = [c for c in all_features if c not in drop_cols]\n",
        "\n",
        "print('Dropped columns (heuristic):', drop_cols)\n",
        "print('Using features:', features[:20])\n",
        "\n",
        "# Split numeric and categorical\n",
        "num_feats = df[features].select_dtypes(include=[np.number]).columns.tolist()\n",
        "cat_feats = [c for c in features if c not in num_feats]\n",
        "\n",
        "print('Numeric:', num_feats)\n",
        "print('Categorical:', cat_feats)\n",
        "\n",
        "# Build preprocessing pipeline (simple, robust)\n",
        "num_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "cat_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', num_pipeline, num_feats),\n",
        "    ('cat', cat_pipeline, cat_feats)\n",
        "], remainder='drop')\n",
        "\n",
        "print('\\nPreprocessor created. Next: fit-transform and ensure no NaNs remain (important!).')\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Cyclical adjustment (placeholder explanation)\n",
        "\n",
        "- For true TTC PD, you should: obtain macro series (GDP, unemployment), compute a credit-cycle index, regress each candidate predictor on the index and keep residuals as TTC predictors.\n",
        "- This cell is a placeholder reminder — actual implementation depends on available macro data and modeling choices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Placeholder: show where cyclical adjustment would be implemented\n",
        "print('If you have macro series, create a dataframe `macro_df` with time index and merge with borrower-level time keys to regress features.')\n",
        "print('Example steps:')\n",
        "print('1. Build macro index e.g., credit_cycle = (normalized GDP * -1) + unemployment')\n",
        "print('2. For each feature, run: feature ~ credit_cycle + other_controls, keep residuals')\n",
        "print('3. Use residuals as TTC features; keep original PIT features separately for comparison')\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Model design & estimation (skeleton)\n",
        "\n",
        "We use a pipeline + logistic regression (interpretable, regulatory-friendly). Alternatives: survival models, hierarchical Bayesian, tree-based ensemble for best performance.\n",
        "We include checks to ensure no NaNs reach the estimator (common cause of errors)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train-test split\n",
        "X = df[features]\n",
        "y = df[TARGET]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "print('Train/test shapes:', X_train.shape, X_test.shape)\n",
        "\n",
        "# Fit preprocessor on training data and assert no NaNs after transform\n",
        "preprocessor.fit(X_train)\n",
        "X_train_t = preprocessor.transform(X_train)\n",
        "X_test_t = preprocessor.transform(X_test)\n",
        "\n",
        "if np.isnan(X_train_t).sum() > 0 or np.isnan(X_test_t).sum() > 0:\n",
        "    raise AssertionError('Missing values remain after preprocessing! Check imputer setup.')\n",
        "else:\n",
        "    print('Preprocessing verified: no NaNs after transform.')\n",
        "\n",
        "# Build final pipeline with logistic regression\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('clf', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "print('Model fitted on training data.')\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Evaluation & validation\n",
        "\n",
        "Compute AUC, ROC curve, classification report. For TTC validation you will also:\n",
        "- Run backtests over vintages\n",
        "- Compute PSI across time\n",
        "- Check coefficient stability by refitting on different historical windows\n",
        "\n",
        "This skeleton shows primary metrics and examples to extend."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Predict & evaluate\n",
        "y_proba = pipeline.predict_proba(X_test)[:,1]\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "print(f'Test ROC AUC: {auc:.4f}')\n",
        "print('\\nClassification report:')\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "# ROC curve plot\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(fpr, tpr, label=f'AUC={auc:.4f}')\n",
        "plt.plot([0,1],[0,1],'k--')\n",
        "plt.xlabel('FPR')\n",
        "plt.ylabel('TPR')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Calibration, PSI & backtesting (placeholders)\n",
        "- Implement decile-level calibration checks\n",
        "- Compute PSI between score distributions across vintages (or train/test)\n",
        "- Backtest predicted vs actual defaults by vintage (important for TTC calibration)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calibration by decile example\n",
        "df_eval = pd.DataFrame({'y_true': y_test.values, 'y_proba': y_proba})\n",
        "df_eval['decile'] = pd.qcut(df_eval['y_proba'].rank(method='first'), 10, labels=False)\n",
        "cal = df_eval.groupby('decile').agg(n=('y_true','size'), actual_rate=('y_true','mean'), avg_proba=('y_proba','mean')).reset_index()\n",
        "display(cal)\n",
        "\n",
        "# PSI function (simple)\n",
        "def psi(expected, actual, buckets=10):\n",
        "    exp_perc, bins = np.histogram(expected, bins=buckets, density=True)\n",
        "    act_perc, _ = np.histogram(actual, bins=bins, density=True)\n",
        "    exp_perc = np.where(exp_perc==0, 1e-6, exp_perc)\n",
        "    act_perc = np.where(act_perc==0, 1e-6, act_perc)\n",
        "    return np.sum((exp_perc - act_perc) * np.log(exp_perc / act_perc))\n",
        "\n",
        "train_scores = pipeline.predict_proba(X_train)[:,1]\n",
        "test_scores = y_proba\n",
        "print('PSI (train vs test):', psi(train_scores, test_scores))\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Save artifacts & documentation\n",
        "- Save trained pipeline (`joblib`) and a CSV of evaluation metrics.\n",
        "- Create `model_card.md` or `MDD.md` describing data, assumptions, limitations, validation results.\n",
        "- Store important plots under `artifacts/charts/`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create artifacts folder and save pipeline\n",
        "art_dir = Path('artifacts')\n",
        "art_dir.mkdir(exist_ok=True)\n",
        "joblib.dump(pipeline, art_dir / 'ttc_pit_pipeline_skeleton.joblib')\n",
        "print('Saved pipeline to', art_dir / 'ttc_pit_pipeline_skeleton.joblib')\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Next steps & governance checklist (short)\n",
        "\n",
        "- Add macro time series and implement cyclical-adjustment (feature residualization) for TTC estimation.\n",
        "- Implement time-vintage backtesting (cohort joins on origination month/year).\n",
        "- Add independent validation: separate validation team or notebook to re-run reproducible checks.\n",
        "- Prepare Model Development Document (MDD) and Validation Report.\n",
        "\n",
        "----\n",
        "If you want, I can now:\n",
        "- (A) Expand any section with working example code (e.g., iterative imputer, SHAP, or vintage backtests), or\n",
        "- (B) Convert this skeleton into a fully runnable notebook with the full EDA and model pipeline filled in for the GiveMeSomeCredit dataset (including saved outputs), or\n",
        "- (C) Provide a PowerShell/Git sequence to add this notebook file to your repo and push to GitHub safely."
      ]
    }
  ]
}
