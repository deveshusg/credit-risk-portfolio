{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01_through_the_cycle_pd.ipynb\n",
        "\n",
        "**Project:** Through-the-Cycle (TTC) PD — `Give Me Some Credit` demo (education / reproducible example)\n",
        "\n",
        "**Purpose & structure**\n",
        "- Demonstrate a full TTC-style PD analysis pipeline on a small public dataset for portfolio learning and interview-defendable reproducibility.\n",
        "- Notebook flow (each section explains *what*, *why*, and *alternatives*):\n",
        "  1. Setup & data load (robust path handling)\n",
        "  2. Exhaustive EDA (univariate, missingness, correlations, target stratification)\n",
        "  3. Data prep & feature engineering (imputation, encoding, scaling, cyclical-adjustment notes)\n",
        "  4. Model training (pipeline + logistic regression for interpretability)\n",
        "  5. Evaluation & validation (AUC, KS, calibration, PSI placeholder)\n",
        "  6. Next steps / governance notes\n",
        "\n",
        "> This notebook is intentionally reproducible: minimal outputs embedded and strict preprocessing to avoid NaNs in models. Save to `notebooks/01_through_the_cycle_pd.ipynb` and run in Colab or locally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Setup & imports\n",
        "\n",
        "Install dependencies if running in a fresh Colab environment. Then import libraries. This cell does not assume heavy packages beyond `pandas` and `sklearn` which are standard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# If running in Colab uncomment these installs (only if missing)\n",
        "# !pip install -q scikit-learn pandas matplotlib seaborn imbalanced-learn\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
        "\n",
        "print('Python version:', sys.version.splitlines()[0])\n",
        "print('pandas:', pd.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data path and robust loading\n",
        "- Primary path: `data/raw/GiveMeSomeCredit/cs-training.csv` (as you said). If not found, the cell will try `data/GiveMeSomeCredit/cs-training.csv` and `data/sample_pit.csv` as fallback (so Colab / binder runs won't fail)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data path (edit if your file is elsewhere)\n",
        "candidates = [\n",
        "    'data/raw/GiveMeSomeCredit/cs-training.csv',\n",
        "    'data/GiveMeSomeCredit/cs-training.csv',\n",
        "    'data/sample_pit.csv'\n",
        "]\n",
        "\n",
        "DATA_PATH = None\n",
        "for p in candidates:\n",
        "    if Path(p).exists():\n",
        "        DATA_PATH = p\n",
        "        break\n",
        "\n",
        "if DATA_PATH is None:\n",
        "    raise FileNotFoundError(\n",
        "        'Could not find dataset. Put GiveMeSomeCredit cs-training.csv at one of: ' + ', '.join(candidates)\n",
        "    )\n",
        "\n",
        "print('Using data file:', DATA_PATH)\n",
        "\n",
        "# Load dataset with appropriate options\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print('Raw shape:', df.shape)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Exhaustive EDA\n",
        "\n",
        "Goal: produce a complete picture of feature distributions, missingness, correlation, and target behavior.\n",
        "We'll examine:\n",
        "- Basic shape and dtypes\n",
        "- Missing value table\n",
        "- Target balance and stratified distributions\n",
        "- Univariate statistics for numeric variables\n",
        "- Correlation matrix and multicollinearity diagnostics\n",
        "- Quick checks for outliers / ranges\n",
        "\n",
        "Comments: For TTC work, focus on features that reflect *borrower fundamentals* rather than purely cyclical indicators. In small demos we still follow same steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic info\n",
        "print('Columns and dtypes:')\n",
        "display(df.dtypes)\n",
        "\n",
        "print('\\nDescribe (numeric):')\n",
        "display(df.describe().T)\n",
        "\n",
        "# Missingness summary\n",
        "missing = df.isnull().sum().sort_values(ascending=False)\n",
        "missing = missing[missing > 0]\n",
        "if len(missing)==0:\n",
        "    print('\\nNo missing values detected in raw file')\n",
        "else:\n",
        "    print('\\nMissing value counts:')\n",
        "    display(missing)\n",
        "\n",
        "# Target distribution - the GiveMeSomeCredit dataset uses 'SeriousDlqin2yrs' as target\n",
        "TARGET = 'SeriousDlqin2yrs' if 'SeriousDlqin2yrs' in df.columns else df.columns[-1]\n",
        "print('\\nTarget column chosen as:', TARGET)\n",
        "print('\\nTarget value counts:')\n",
        "display(df[TARGET].value_counts(dropna=False))\n",
        "\n",
        "# Plot target balance\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x=TARGET, data=df)\n",
        "plt.title('Target balance')\n",
        "plt.show()\n",
        "\n",
        "# Correlation heatmap (numeric only)\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if len(num_cols) > 1:\n",
        "    plt.figure(figsize=(10,8))\n",
        "    sns.heatmap(df[num_cols].corr(), annot=True, fmt='.2f', cmap='coolwarm', vmin=-1, vmax=1)\n",
        "    plt.title('Numeric correlation matrix')\n",
        "    plt.show()\n",
        "\n",
        "# Quick univariate distributions for numeric features\n",
        "for c in num_cols:\n",
        "    if c == TARGET:\n",
        "        continue\n",
        "    plt.figure(figsize=(6,3))\n",
        "    sns.histplot(df[c].dropna(), kde=False, bins=40)\n",
        "    plt.title(f'Distribution: {c}')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Data Preparation & Feature Engineering\n",
        "\n",
        "Plan (keeps TTC focus):\n",
        "- Minimal, defensible feature set derived from the dataset (no heavy cyclical signals here).\n",
        "- Impute missing numeric values with median (robust) and categorical with mode.\n",
        "- Scale numeric features for logistic regression.\n",
        "- Use a `ColumnTransformer` + `Pipeline` so preprocessing is encapsulated and reproducible.\n",
        "\n",
        "Alternative choices:\n",
        "- Use model-based imputation (IterativeImputer) for more accuracy.\n",
        "- If building true TTC, regress features on macro index and keep residuals; here we place a placeholder for that step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify feature columns (simple heuristic)\n",
        "all_cols = df.columns.tolist()\n",
        "if TARGET in all_cols:\n",
        "    feature_cols = [c for c in all_cols if c != TARGET]\n",
        "else:\n",
        "    # fallback - assume last column is target\n",
        "    feature_cols = all_cols[:-1]\n",
        "\n",
        "# split numeric and categorical\n",
        "numeric_features = df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_features = [c for c in feature_cols if c not in numeric_features]\n",
        "\n",
        "print('Numeric features:', numeric_features)\n",
        "print('Categorical features:', categorical_features)\n",
        "\n",
        "# Build preprocessing pipeline\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ],\n",
        "    remainder='drop'  # drop any other columns\n",
        ")\n",
        "\n",
        "# Create full pipeline with model\n",
        "clf = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', LogisticRegression(max_iter=1000, solver='lbfgs'))\n",
        "])\n",
        "\n",
        "# Train-test split stratified by target\n",
        "X = df[feature_cols]\n",
        "y = df[TARGET]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print('Train shape:', X_train.shape, 'Test shape:', X_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Sanity checks before fitting\n",
        "- We'll fit the pipeline directly which handles imputation; but we also perform an explicit transform to assert *no missing values remain* after preprocessing.\n",
        "- This prevents the earlier `ValueError: Input X contains NaN` which occurs when models get NaNs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit only the preprocessor to training data and transform both train & test\n",
        "preprocessor.fit(X_train)\n",
        "X_train_t = preprocessor.transform(X_train)\n",
        "X_test_t = preprocessor.transform(X_test)\n",
        "\n",
        "# After transform, ensure no NaNs remain\n",
        "nans_train = np.isnan(X_train_t).sum()\n",
        "nans_test = np.isnan(X_test_t).sum()\n",
        "print('NaNs after preprocessing (train):', int(nans_train))\n",
        "print('NaNs after preprocessing (test):', int(nans_test))\n",
        "\n",
        "if nans_train != 0 or nans_test != 0:\n",
        "    # helpful debug: show columns with missing values before transform\n",
        "    print('\\nColumns with missing values before preprocessing (train):')\n",
        "    print(X_train.isnull().sum()[X_train.isnull().sum() > 0])\n",
        "    raise AssertionError('⚠️ Missing values remain after preprocessing! Check imputer configuration.')\n",
        "else:\n",
        "    print('\\n✅ No missing values remain after preprocessing. Ready to fit model.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Model training\n",
        "\n",
        "Train the logistic regression within the pipeline. We keep logistic for interpretability; alternatives may include tree-based models (LightGBM) which can handle missing values natively and often produce better raw performance.\n",
        "We also show simple cross-validated AUC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit full pipeline on training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict & evaluate\n",
        "y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f'ROC AUC (test): {auc:.4f}')\n",
        "\n",
        "print('\\nClassification report (test):')\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "# ROC curve plot\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(fpr, tpr, label=f'AUC = {auc:.4f}')\n",
        "plt.plot([0,1],[0,1],'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Simple calibration & stability checks\n",
        "\n",
        "This section provides quick sanity checks: calibration by deciles, and a placeholder for Population Stability Index (PSI) between train & test or across vintages. For formal governance, you'd perform backtests by vintage and long-run calibration to TTC-levels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calibration by decile\n",
        "df_eval = pd.DataFrame({'y_true': y_test, 'y_proba': y_pred_proba})\n",
        "df_eval['decile'] = pd.qcut(df_eval['y_proba'].rank(method='first'), 10, labels=False)\n",
        "cal = df_eval.groupby('decile').agg(n=('y_true','size'),\n",
        "                                     actual_default_rate=('y_true','mean'),\n",
        "                                     avg_pred_proba=('y_proba','mean')).reset_index()\n",
        "display(cal)\n",
        "\n",
        "# Quick PSI placeholder function (train vs test distributions for score)\n",
        "def psi(expected, actual, buckets=10):\n",
        "    # expected/actual are 1d arrays\n",
        "    exp_perc, bins = np.histogram(expected, bins=buckets, density=True)\n",
        "    act_perc, _ = np.histogram(actual, bins=bins, density=True)\n",
        "    # avoid zeros\n",
        "    exp_perc = np.where(exp_perc==0, 1e-6, exp_perc)\n",
        "    act_perc = np.where(act_perc==0, 1e-6, act_perc)\n",
        "    return np.sum((exp_perc - act_perc) * np.log(exp_perc / act_perc))\n",
        "\n",
        "train_scores = clf.predict_proba(X_train)[:,1]\n",
        "test_scores = y_pred_proba\n",
        "psi_val = psi(train_scores, test_scores, buckets=10)\n",
        "print(f'PSI (train vs test): {psi_val:.6f}  (small -> stable; >0.25 suggests shift)')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Save artefacts and next steps\n",
        "\n",
        "- Save the trained pipeline for reproducibility and deployment.\n",
        "- Document assumptions: imputation strategy, variable selection, and the fact this is an educational TTC-style workflow on a small dataset.\n",
        "- Next steps for true TTC: obtain macro series, model feature → macro regression to remove cyclical component (keep residuals), re-estimate long-run PD calibration using multi-year vintages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save pipeline artifact\n",
        "import joblib\n",
        "ARTIFACTS_DIR = Path('artifacts')\n",
        "ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
        "joblib.dump(clf, ARTIFACTS_DIR / 'ttc_pit_pipeline_lgr.joblib')\n",
        "print('Saved pipeline to', ARTIFACTS_DIR / 'ttc_pit_pipeline_lgr.joblib')\n",
        "\n",
        "# Final message\n",
        "print('\\nDone. Notebook ran end-to-end with imputation & pipeline to avoid NaN issues in modeling.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Suggestions to improve this notebook\n",
        "1. Replace simple median imputation with `IterativeImputer` or model-based imputation for better fidelity.\n",
        "2. Use cross-validation with time-based folds if you have time-series/vintage structure.\n",
        "3. For TTC adjustment: collect macro series (GDP, unemployment), build a credit-cycle index, regress features on the cycle and keep residuals for TTC model training.\n",
        "4. Add model explainability: coefficient table for logistic regression, SHAP for tree models.\n",
        "5. Implement model validation docs (MDD) and automated unit tests for data pipelines."
      ]
    }
  ]
}
