# ---
# üßÆ Through-The-Cycle (TTC) Probability of Default Model
# Dataset: Give Me Some Credit (Kaggle)
# ---

# =========================================================
# 1Ô∏è‚É£ PROJECT OVERVIEW
# =========================================================
"""
Objective:
-----------
Build a Through-the-Cycle (TTC) Probability of Default (PD) model 
using the ‚ÄúGive Me Some Credit‚Äù dataset. TTC PDs represent a 
borrower‚Äôs average default risk over an economic cycle, 
removing temporary macroeconomic effects.

Why TTC PD?
------------
Unlike Point-in-Time (PIT) models, TTC PDs are:
- More stable across business cycles
- Used in Basel IRB frameworks and IFRS9 regulatory capital models
- Useful for long-term risk assessment and capital allocation

Notebook Outline:
-----------------
1. Load dataset (auto-download from GitHub if needed)
2. Detailed EDA ‚Äì structure, missingness, distributions, correlations
3. Data preparation & feature engineering
4. Logistic Regression model (baseline TTC)
5. Model evaluation & validation
6. TTC PD calibration
7. Next steps & improvement ideas
"""

# =========================================================
# 2Ô∏è‚É£ LIBRARY IMPORTS & DATA LOADING
# =========================================================

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

warnings.filterwarnings("ignore")
sns.set(style="whitegrid", palette="muted")

# --- Define dataset path ---
DATA_PATH = "data/raw/GiveMeSomeCredit/cs-training.csv"

# --- Auto-download dataset if not found ---
if not os.path.exists(DATA_PATH):
    print("‚ö†Ô∏è Local file not found. Downloading from GitHub...")
    os.makedirs("data/raw/GiveMeSomeCredit", exist_ok=True)
    url = "https://raw.githubusercontent.com/deveshusg/credit-risk-portfolio/main/data/raw/GiveMeSomeCredit/cs-training.csv"
    df = pd.read_csv(url)
    df.to_csv(DATA_PATH, index=False)
else:
    df = pd.read_csv(DATA_PATH)

print(f"‚úÖ Dataset loaded successfully with shape {df.shape}")
df.head()

# =========================================================
# 3Ô∏è‚É£ INITIAL DATA UNDERSTANDING
# =========================================================

print("=== Dataset Info ===")
df.info()

print("\n=== Missing Values Summary ===")
missing_summary = df.isnull().sum().sort_values(ascending=False)
missing_pct = (df.isnull().mean() * 100).sort_values(ascending=False)
pd.DataFrame({"Missing Count": missing_summary, "Missing %": missing_pct}).head(10)

# Numeric summary
print("\n=== Descriptive Statistics ===")
df.describe().T

# =========================================================
# 4Ô∏è‚É£ EXPLORATORY DATA ANALYSIS (EDA)
# =========================================================

target = "SeriousDlqin2yrs"

# --- Target distribution ---
print("\nTarget distribution:")
print(df[target].value_counts(normalize=True).round(3))
sns.countplot(x=target, data=df)
plt.title("Target Distribution: SeriousDlqin2yrs")
plt.show()

# --- Numeric feature histograms ---
num_cols = df.select_dtypes(include=[np.number]).columns.drop(target)
df[num_cols].hist(bins=30, figsize=(16, 12), color="cornflowerblue")
plt.suptitle("Numeric Feature Distributions", fontsize=16)
plt.show()

# --- Outlier detection ---
outlier_ratio = {}
for col in num_cols:
    q1, q3 = np.percentile(df[col].dropna(), [25, 75])
    iqr = q3 - q1
    lower, upper = q1 - 1.5 * iqr, q3 + 1.5 * iqr
    outliers = ((df[col] < lower) | (df[col] > upper)).sum()
    outlier_ratio[col] = round(outliers / len(df) * 100, 2)

print("\nOutlier % by column:")
pd.DataFrame({"Outlier %": outlier_ratio}).sort_values("Outlier %", ascending=False)

# --- Correlation matrix ---
plt.figure(figsize=(10, 8))
corr = df[num_cols].corr()
sns.heatmap(corr, annot=True, cmap="coolwarm", center=0)
plt.title("Feature Correlation Matrix")
plt.show()

# --- Feature vs Target KDE plots ---
for col in num_cols:
    plt.figure(figsize=(6, 4))
    sns.kdeplot(x=df[col], hue=df[target], fill=True, common_norm=False, alpha=0.6)
    plt.title(f"Distribution of {col} by Default Status")
    plt.show()

# =========================================================
# 5Ô∏è‚É£ DATA PREPARATION & FEATURE ENGINEERING
# =========================================================

"""
Why these steps?
----------------
- Handle missing values (Median imputation for robustness)
- Cap extreme outliers (Winsorization at 1st/99th percentile)
- Derive interpretable variables (AgeBucket, IncomeToDebt)
- Standardize numeric columns for logistic regression stability
"""

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

df_prep = df.copy()

# --- Handle missing values ---
imputer = SimpleImputer(strategy="median")
num_cols = df_prep.select_dtypes(include=[np.number]).columns
df_prep[num_cols] = imputer.fit_transform(df_prep[num_cols])

print(f"‚úÖ Missing values after imputation: {df_prep.isnull().sum().sum()}")

# --- Cap extreme outliers ---
for col in ["RevolvingUtilizationOfUnsecuredLines", "DebtRatio"]:
    df_prep[col] = np.clip(
        df_prep[col], df_prep[col].quantile(0.01), df_prep[col].quantile(0.99)
    )

# --- Feature engineering ---
df_prep["AgeBucket"] = pd.cut(
    df_prep["age"], bins=[20, 30, 40, 50, 60, 70, 80, 100], labels=False
)
df_prep["IncomeToDebt"] = df_prep["MonthlyIncome"] / (df_prep["DebtRatio"] + 1e-6)

# --- Scaling ---
scaler = StandardScaler()
scaled_cols = [
    "RevolvingUtilizationOfUnsecuredLines",
    "DebtRatio",
    "MonthlyIncome",
    "IncomeToDebt",
]
df_prep[scaled_cols] = scaler.fit_transform(df_prep[scaled_cols])

print(f"‚úÖ Data prepared. Shape: {df_prep.shape}")

# =========================================================
# 6Ô∏è‚É£ MODEL DEVELOPMENT (Logistic Regression TTC PD)
# =========================================================

"""
Why Logistic Regression?
-------------------------
- Transparent and interpretable coefficients
- Produces calibrated probabilities of default (PD)
- Industry-standard for retail credit risk models

Alternative models:
--------------------
- Tree-based models (e.g., XGBoost) handle non-linearity
  but are less interpretable.
- For TTC modeling, interpretability and explainability
  are prioritized over marginal accuracy gains.
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, classification_report

X = df_prep.drop(columns=[target])
y = df_prep[target]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)

model = LogisticRegression(max_iter=500)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]

print("‚úÖ Model training complete.")
