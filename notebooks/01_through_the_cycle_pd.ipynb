{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "colab": {
   "name": "01_through_the_cycle_pd.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 — Through-the-Cycle (TTC) PD Model\n",
    "\n",
    "**Author:** Devesh Gupta  \n",
    "**Purpose:** Build a defendable TTC PD modelling notebook following the portfolio flow: Data Strategy → EDA → Feature Engineering → Modelling → Calibration → Validation → Governance → Monitoring.\n",
    "\n",
    "> **Note / assumption:** I do **not** have access to your internal systems or production datasets. This notebook uses *placeholder file paths* under `data/`. Replace those CSVs with your actual internal exports (application data, repayment history, bureau scores, and macro series). Any time I make an assumption (file names, column names, default flags), I mark it clearly so you can change it to your real field names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick instructions before running\n",
    "1. Upload your actual CSVs to the repository `data/` folder or change the `DATA_*` path variables below.\n",
    "2. Required inputs (example names used below — **replace** if different):\n",
    "   - `data/applications.csv` — borrower/application-level static fields (customer_id, origination_date, annual_income, credit_limit, etc.)\n",
    "   - `data/repayments.csv` — transaction-level / performance table (customer_id, txn_date, dpd, etc.) or precomputed flag `dpd_90_within_12m`\n",
    "   - `data/bureau.csv` — external bureau scores (customer_id, bureau_score, bureau_date) — optional but highly recommended\n",
    "   - `data/macro.csv` — monthly macro indicators (date, gdp_growth, unemployment, cpi, etc.)\n",
    "3. Open the notebook in Colab: `File -> Save a copy in Drive` then run cells top → bottom. Replace field names where noted."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Imports & global config\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid', context='notebook')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import calibration_curve\n",
    "import statsmodels.api as sm\n",
    "RND = 42\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.width', 140)\n",
    "print('Python', sys.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Strategy & Governance (Load + basic checks)\n",
    "\n",
    "*Purpose:* establish defensible inputs and basic DQ checks. Replace the example filenames with your real exports."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === CONFIG: update these to match your repo/data filenames ===\n",
    "DATA_APPLICATIONS = 'data/applications.csv'          # static application/profile data\n",
    "DATA_REPAYMENTS = 'data/repayments.csv'            # transaction or performance table OR precomputed flags\n",
    "DATA_BUREAU = 'data/bureau.csv'                    # optional: bureau scores\n",
    "DATA_MACRO = 'data/macro.csv'                      # monthly macro indicators (date, gdp_growth, unemployment...)\n",
    "# =================================================================\n",
    "\n",
    "def must_read(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"File not found: {path} — please upload to repo data/ or change path in the config cell.\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "# Try loading with defensive messages\n",
    "for p in [DATA_APPLICATIONS, DATA_REPAYMENTS, DATA_MACRO]:\n",
    "    if not os.path.exists(p):\n",
    "        print(f\"WARNING: {p} not found. Replace with your actual file or upload sample. This notebook will still show code examples.\")\n",
    "\n",
    "# Load when available\n",
    "apps = pd.read_csv(DATA_APPLICATIONS, parse_dates=['origination_date']) if os.path.exists(DATA_APPLICATIONS) else pd.DataFrame()\n",
    "repay = pd.read_csv(DATA_REPAYMENTS, parse_dates=['txn_date']) if os.path.exists(DATA_REPAYMENTS) else pd.DataFrame()\n",
    "macro = pd.read_csv(DATA_MACRO, parse_dates=['date']) if os.path.exists(DATA_MACRO) else pd.DataFrame()\n",
    "bureau = pd.read_csv(DATA_BUREAU, parse_dates=['bureau_date']) if os.path.exists(DATA_BUREAU) else pd.DataFrame()\n",
    "\n",
    "print('applications rows:', len(apps), 'repayments rows:', len(repay), 'macro rows:', len(macro), 'bureau rows:', len(bureau))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Scoping, default definition & time windows (edit as required)\n",
    "\n",
    "Assumptions in this notebook (edit below if your policies differ):\n",
    "- Portfolio: retail unsecured (single facility per `customer_id` assumed)\n",
    "- Default definition: `90+ DPD within 12 months of origination` mapped to `dpd_90_within_12m` flag OR computed from `repay` table.\n",
    "- Performance horizon: 12 months from origination (you can change to 24/36 months by editing code where `12` appears)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### Create a modeling dataframe `df` that merges required inputs and builds the label\n",
    "if apps.empty:\n",
    "    print('No applications file loaded — creating a small synthetic demo sample so rest of notebook runs. Replace with your real data.')\n",
    "    # Minimal demo data (only for demonstration). Replace with your real exports.\n",
    "    apps = pd.DataFrame({\n",
    "        'customer_id':[1,2,3,4,5,6,7,8,9,10],\n",
    "        'origination_date':pd.to_datetime(['2018-01-01','2018-02-10','2018-03-01','2019-06-05','2019-06-10','2020-01-05','2020-02-20','2021-03-03','2021-05-05','2021-07-01']),\n",
    "        'annual_income':[40000,30000,120000,55000,45000,38000,60000,48000,130000,75000],\n",
    "        'credit_limit':[20000,15000,50000,25000,20000,15000,60000,30000,80000,40000],\n",
    "        'delinq_2yrs':[0,1,0,2,0,0,3,0,0,1]\n",
    "    })\n",
    "if repay.empty:\n",
    "    # Synthetic flag column: dpd_90_within_12m\n",
    "    # In your real data, compute this by looking at repayment DPDs within the 12 month window from origination.\n",
    "    apps['dpd_90_within_12m'] = [0,1,0,0,1,0,0,0,1,0]\n",
    "    df = apps.copy()\n",
    "else:\n",
    "    # Example: if you have repayment rows with dpd or status, you'd compute the 90+ DPD within 12 months here.\n",
    "    # This code assumes a simple pre-computed flag exists in repay or apps. If not, replace with your label-building logic.\n",
    "    if 'dpd_90_within_12m' in apps.columns:\n",
    "        df = apps.copy()\n",
    "    else:\n",
    "        # fallback: try to aggregate repay to compute the label (example pattern)\n",
    "        print('Computing dpd_90_within_12m from repayments table — adjust logic for your schema')\n",
    "        repay['orig_month'] = repay['txn_date'].dt.to_period('M').astype(str)\n",
    "        # Example aggregation: max dpd by customer in 12 months after orig\n",
    "        # NOTE: Replace with your real orig date join logic\n",
    "        df = apps.merge(repay.groupby('customer_id').agg(max_dpd=('dpd','max')).reset_index(), on='customer_id', how='left')\n",
    "        df['dpd_90_within_12m'] = (df['max_dpd'] >= 90).astype(int)\n",
    "\n",
    "df['orig_month'] = pd.to_datetime(df['origination_date']).dt.to_period('M').astype(str)\n",
    "print('Modeling df sample:')\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "Understand cohort volumes, observed default rates, and relationships with candidate predictors."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cohort size and observed default rate\n",
    "if 'dpd_90_within_12m' not in df.columns:\n",
    "    raise KeyError('Label dpd_90_within_12m not found in df — compute it from repayments or add the flag to apps file.')\n",
    "cohort = (df.groupby('orig_month')\n",
    "            .agg(n=('customer_id','size'),\n",
    "                 default_rate=('dpd_90_within_12m','mean'))\n",
    "            .reset_index().sort_values('orig_month'))\n",
    "print('Cohort summary (first 20 rows):')\n",
    "display(cohort.head(20))\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(cohort['orig_month'], cohort['default_rate'], marker='o')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Observed Default Rate by Origination Month')\n",
    "plt.ylabel('Default rate')\n",
    "plt.xlabel('Origination month')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Univariate checks for a few candidate variables\n",
    "# Example variables: annual_income, delinq_2yrs, credit_limit\n",
    "def binned_default_rate(df, col, bins=5):\n",
    "    try:\n",
    "        df['_b'] = pd.qcut(df[col].rank(method='first'), q=bins, duplicates='drop')\n",
    "    except Exception:\n",
    "        df['_b'] = pd.cut(df[col], bins=bins)\n",
    "    out = df.groupby('_b').agg(n=('customer_id','size'), default_rate=('dpd_90_within_12m','mean'))\n",
    "    return out\n",
    "\n",
    "for c in ['annual_income','credit_limit','delinq_2yrs']:\n",
    "    if c in df.columns:\n",
    "        print('\\nVariable:', c)\n",
    "        display(binned_default_rate(df, c, bins=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Transformation & Feature Engineering\n",
    "Derive stable predictors and perform cyclical adjustment (example: remove relationship with GDP from a behavioral variable)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create basic transformed features (edit names to your real columns)\n",
    "df['log_income'] = np.log1p(df['annual_income']) if 'annual_income' in df.columns else 0\n",
    "df['util_rate'] = (df['credit_limit'] - 0) / df['credit_limit'] if 'credit_limit' in df.columns else 0  # placeholder; replace with revolving_balance/credit_limit\n",
    "df['past_delinquency_flag'] = (df['delinq_2yrs'] > 0).astype(int) if 'delinq_2yrs' in df.columns else 0\n",
    "\n",
    "# Merge macro at orig_month for cyclical adjustment — requires macro file with 'date' column at monthly frequency\n",
    "if not macro.empty:\n",
    "    macro['orig_month'] = macro['date'].dt.to_period('M').astype(str)\n",
    "    # choose one macro variable for demo (gdp_growth). Replace with your macro names.\n",
    "    if 'gdp_growth' not in macro.columns:\n",
    "        # if macro doesn't have gdp_growth, try any numeric column\n",
    "        numeric_macros = macro.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        if numeric_macros:\n",
    "            chosen_macro = numeric_macros[0]\n",
    "            print('Using macro column', chosen_macro, 'for cyclical adjustment (no gdp_growth column found).')\n",
    "            macro = macro.rename(columns={chosen_macro: 'gdp_growth'})\n",
    "        else:\n",
    "            print('Macro file loaded but contains no numeric columns. Skipping cyclical adjustment demo.')\n",
    "    df = df.merge(macro[['orig_month','gdp_growth']].drop_duplicates(), on='orig_month', how='left')\n",
    "else:\n",
    "    df['gdp_growth'] = 0\n",
    "\n",
    "# Demonstration: cyclical adjustment — regress util_rate on gdp_growth and retain residuals as cycle-removed feature\n",
    "if 'util_rate' in df.columns:\n",
    "    Xc = sm.add_constant(df['gdp_growth'].fillna(0))\n",
    "    yc = df['util_rate'].fillna(0)\n",
    "    cyc_mod = sm.OLS(yc, Xc).fit()\n",
    "    df['util_rate_resid'] = cyc_mod.resid\n",
    "    print('Cyclical regression summary (util_rate ~ gdp_growth):')\n",
    "    print(cyc_mod.summary().tables[1])\n",
    "else:\n",
    "    df['util_rate_resid'] = 0\n",
    "\n",
    "display(df[[col for col in df.columns if col.startswith(('log_income','util_rate','past_delinquency','gdp_growth','util_rate_resid'))]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Design & Estimation\n",
    "Logistic regression for interpretability. For TTC we exclude contemporaneous macro dummies and use cycle-removed predictors (residuals) and/or set macro variables to long-run means during calibration."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Prepare X and y; choose features that are stable (cycle-adjusted where applicable)\n",
    "features = []\n",
    "if 'log_income' in df.columns:\n",
    "    features.append('log_income')\n",
    "if 'util_rate_resid' in df.columns:\n",
    "    features.append('util_rate_resid')\n",
    "if 'past_delinquency_flag' in df.columns:\n",
    "    features.append('past_delinquency_flag')\n",
    "\n",
    "if not features:\n",
    "    raise ValueError('No modelling features found in df — please create features or edit the feature list.')\n",
    "\n",
    "X = df[features].fillna(0)\n",
    "y = df['dpd_90_within_12m'].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=RND, stratify=y)\n",
    "print('Train shape:', X_train.shape, 'Test shape:', X_test.shape)\n",
    "\n",
    "model = LogisticRegression(class_weight='balanced', solver='liblinear', random_state=RND)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "brier = brier_score_loss(y_test, y_pred_proba)\n",
    "print(f'AUC: {auc:.4f}    Brier: {brier:.4f}')\n",
    "\n",
    "coef_df = pd.DataFrame({'feature':features, 'coef': model.coef_.ravel()})\n",
    "display(coef_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calibration & Scaling (map model mean to long-run PD)\n",
    "We often want TTC PDs aligned to a long-run observed average default rate. Here is a simple scaling example — more sophisticated approaches (Platt scaling, isotonic regression, grade-level scaling) are recommended for production."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Example: compute long-run observed average default (user should compute using >=1 full cycle of history)\n",
    "observed_long_run_pd = None\n",
    "if 'dpd_90_within_12m' in df.columns:\n",
    "    observed_long_run_pd = df['dpd_90_within_12m'].mean()\n",
    "    print('Observed long-run PD (naive):', observed_long_run_pd)\n",
    "else:\n",
    "    observed_long_run_pd = 0.03  # default assumption — change to your long-run value\n",
    "    print('Using assumed long-run PD =', observed_long_run_pd)\n",
    "\n",
    "# Current model average predicted PD on the test set\n",
    "raw_avg = y_pred_proba.mean()\n",
    "print('Model raw average PD on test set:', raw_avg)\n",
    "\n",
    "if raw_avg > 0:\n",
    "    scale_factor = observed_long_run_pd / raw_avg\n",
    "else:\n",
    "    scale_factor = 1.0\n",
    "print('Scale factor to align to long-run PD:', round(scale_factor,4))\n",
    "\n",
    "df_test = X_test.copy()\n",
    "df_test['pd_raw'] = model.predict_proba(X_test)[:,1]\n",
    "df_test['pd_scaled'] = df_test['pd_raw'] * scale_factor\n",
    "display(df_test[['pd_raw','pd_scaled']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Validation\n",
    "Discrimination (AUC), calibration plot, and placeholders for PSI / backtests."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calibration curve\n",
    "prob_true, prob_pred = calibration_curve(y_test, y_pred_proba, n_bins=10)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(prob_pred, prob_true, marker='o')\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('Mean predicted probability')\n",
    "plt.ylabel('Observed fraction of positives')\n",
    "plt.title('Calibration plot')\n",
    "plt.show()\n",
    "\n",
    "# AUC printed earlier; KS and Gini can be added. Example: compute simple KS\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thr = roc_curve(y_test, y_pred_proba)\n",
    "ks = max(abs(tpr - fpr))\n",
    "print('KS:', round(ks,4), 'AUC:', round(auc,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Population Stability Index (PSI) helper — compare distribution in train vs. current / baseline\n",
    "def psi(expected, actual, buckets=10):\n",
    "    expected = np.asarray(expected)\n",
    "    actual = np.asarray(actual)\n",
    "    breakpoints = np.percentile(expected, np.linspace(0,100,buckets+1))\n",
    "    eps = 1e-6\n",
    "    def wgt_pct(arr, low, high):\n",
    "        return ((arr >= low) & (arr < high)).sum() / len(arr)\n",
    "    psi_val = 0\n",
    "    for i in range(len(breakpoints)-1):\n",
    "        e = wgt_pct(expected, breakpoints[i], breakpoints[i+1]) + eps\n",
    "        a = wgt_pct(actual, breakpoints[i], breakpoints[i+1]) + eps\n",
    "        psi_val += (e - a) * np.log(e / a)\n",
    "    return psi_val\n",
    "\n",
    "psi_val = psi(X_train['log_income'] if 'log_income' in X_train else X_train.iloc[:,0],\n",
    "              X_test['log_income'] if 'log_income' in X_test else X_test.iloc[:,0], buckets=10)\n",
    "print('Example PSI on log_income (train vs test):', round(psi_val,4))\n",
    "\n",
    "# Backtesting / vintage checks placeholder — implement based on your business vintage definitions\n",
    "print('Backtesting: compare predicted vs observed defaults by origination vintage — implement using your production vintage logic.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Documentation & Governance (Checklist)\n",
    "- Model Development Document (MDD): record data sources, feature engineering, model specification, assumptions, and limitations.  \n",
    "- Validation Report: independent tests and backtests; record KS, AUC, calibration, PSI, coefficient stability.  \n",
    "- Version control: commit code + data snapshot (where permitted).  \n",
    "- Sign-offs: model owner, validator, governance committee.  \n",
    "- Data lineage: store copies or reproducible extraction scripts with timestamps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Implementation & Monitoring (Operational notes)\n",
    "- Score export: save `pd_scaled` per customer to `artifacts/score_YYYYMMDD.csv` for ingestion (example code below).  \n",
    "- Monitoring: create monthly dashboard tracking AUC, KS, PSI, calibration drift, and changes to long-run PD.  \n",
    "- Recalibration triggers: material performance deterioration (AUC drop > X), PSI > 0.2 on key variables, or a regime change in macro indicators.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Example: export scaled PDs for the test slice (production integration step)\n",
    "out = df_test.copy()\n",
    "out = out.reset_index().rename(columns={'index':'_orig_index'})\n",
    "out['customer_id'] = df.loc[out['_orig_index'], 'customer_id'].values if '_orig_index' in out.columns else None\n",
    "OUT_PATH = 'artifacts/pd_scores_sample.csv'\n",
    "os.makedirs('artifacts', exist_ok=True)\n",
    "out[['pd_raw','pd_scaled']].to_csv(OUT_PATH, index=False)\n",
    "print('Wrote sample PD scores to', OUT_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary & Next steps\n",
    "- This notebook is an end-to-end TTC PD *template*. Replace placeholder filenames and column names with your real data exports.  \n",
    "- Next notebook (PiT PD) will follow the same structure but keep macro variables to current values and include time-series features and survival modelling options.  \n",
    "- If you want, I can convert this JSON into the actual `.ipynb` file content and paste it into your repo file content for you to commit (or provide the raw JSON as a downloadable file)."
   ]
  }
 ]
}
