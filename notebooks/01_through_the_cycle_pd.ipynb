{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 01 â€” Through-the-Cycle (TTC) PD (Give Me Some Credit)\n",
    "\n",
    "**Overview**\n",
    "\n",
    "- Objective: Build a defensible TTC-style PD workflow on the \"Give Me Some Credit\" dataset. This notebook is structured for Colab/Binder and follows the portfolio flow: Data load â†’ EDA â†’ Robust preprocessing (TTC-focused) â†’ Model training â†’ Evaluation â†’ Save artefacts.\n",
    "\n",
    "### Notes before running\n",
    "- Expected dataset path: `data/raw/GiveMeSomeCredit/cs-training.csv`. Edit `DATA_PATH` below if necessary.\n",
    "- This notebook focuses on robust preprocessing and avoids the NaN pitfalls encountered earlier. If you run into any column-specific NaNs, the preprocessing cell prints a breakdown so we can iterate fast.\n",
    "\n",
    "### What this notebook contains (short)\n",
    "1. Imports & data load (graceful if file missing)\n",
    "2. Quick but informative EDA (head, types, missingness, simple univariate checks)\n",
    "3. Robust preprocessing & feature engineering (imputation, outlier capping, AgeBucket handling, scaling). This is the updated cell you asked for.\n",
    "4. Model pipeline (imputer + scaler + logistic regression) and evaluation (AUC, classification report)\n",
    "5. Save a small model artifact to `artifacts/` if available\n",
    "\n",
    "---\n",
    "\n",
    "If anything breaks, **copy the exact traceback** and paste it here â€” I'll patch immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 1) Imports and load dataset\n",
    "# --------------------------\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# Path to data (edit if your file is elsewhere)\n",
    "DATA_PATH = 'data/raw/GiveMeSomeCredit/cs-training.csv'\n",
    "\n",
    "print(f'Looking for dataset at: {DATA_PATH}')\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(f\"Dataset not found at {DATA_PATH}. Please upload the file to that path or edit DATA_PATH.\")\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print('\\nLoaded dataset. Shape:', df.shape)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 2) Quick EDA (types, missingness, basic univariate checks)\n",
    "# --------------------------\n",
    "print('\\nColumn data types and non-null counts:')\n",
    "display(df.info())\n",
    "\n",
    "print('\\nMissing values per column:')\n",
    "display(df.isnull().sum().sort_values(ascending=False))\n",
    "\n",
    "print('\\nBasic descriptive stats for numeric columns:')\n",
    "display(df.describe().T)\n",
    "\n",
    "# Target column check\n",
    "target_col = 'SeriousDlqin2yrs'  # GiveMeSomeCredit target\n",
    "if target_col not in df.columns:\n",
    "    raise KeyError(f\"Expected target column '{target_col}' not found in data. Columns: {list(df.columns)}\")\n",
    "\n",
    "print(f\"\\nTarget balance (value counts) for '{target_col}':\")\n",
    "display(df[target_col].value_counts(dropna=False))\n",
    "\n",
    "# Quick univariate: default rate by simple age bins (demonstrative)\n",
    "if 'age' in df.columns:\n",
    "    display(df.groupby(pd.cut(df['age'], bins=[0,30,40,50,60,70,100]))[target_col].mean())\n",
    "\n",
    "print('\\nEDA complete â€” proceed to preprocessing cell below.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# UPDATED Data Preparation & Feature Engineering (robust)\n",
    "# - fixes post-imputation NaNs (pd.cut, derived cols, etc.)\n",
    "# - prints detailed missing-col breakdown if assertion would fail\n",
    "# =========================================================\n",
    "df_prep = df.copy()\n",
    "\n",
    "# --- 1) Impute numeric columns robustly ---\n",
    "numeric_cols = df_prep.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if target_col in numeric_cols:\n",
    "    numeric_cols.remove(target_col)\n",
    "\n",
    "# Use median imputation (robust to outliers)\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "df_prep[numeric_cols] = num_imputer.fit_transform(df_prep[numeric_cols])\n",
    "missing_after_impute = int(df_prep[numeric_cols].isnull().sum().sum())\n",
    "print(f'ðŸ” Missing values in numeric cols after median imputation: {missing_after_impute}')\n",
    "\n",
    "# --- 2) Cap extreme outliers on important numeric features ---\n",
    "for col in ['RevolvingUtilizationOfUnsecuredLines', 'DebtRatio']:\n",
    "    if col in df_prep.columns:\n",
    "        low = df_prep[col].quantile(0.01)\n",
    "        high = df_prep[col].quantile(0.99)\n",
    "        df_prep[col] = np.clip(df_prep[col], low, high)\n",
    "\n",
    "# --- 3) Feature engineering (careful to avoid NaNs) ---\n",
    "if 'age' in df_prep.columns:\n",
    "    min_age = int(max(0, df_prep['age'].min() - 1))\n",
    "    max_age = int(df_prep['age'].max() + 1)\n",
    "    bins = [min_age, 30, 40, 50, 60, 70, 80, max_age]\n",
    "    df_prep['AgeBucket'] = pd.cut(df_prep['age'], bins=bins, labels=False, include_lowest=True)\n",
    "    df_prep['AgeBucket'] = df_prep['AgeBucket'].fillna(-1).astype(int)\n",
    "\n",
    "if {'MonthlyIncome', 'DebtRatio'}.issubset(df_prep.columns):\n",
    "    eps = 1e-8\n",
    "    df_prep['IncomeToDebt'] = df_prep['MonthlyIncome'] / (df_prep['DebtRatio'] + eps)\n",
    "\n",
    "# --- 4) Scaling selected numeric features (StandardScaler) ---\n",
    "scaled_cols = [c for c in ['RevolvingUtilizationOfUnsecuredLines','DebtRatio','MonthlyIncome','IncomeToDebt'] if c in df_prep.columns]\n",
    "if scaled_cols:\n",
    "    scaler = StandardScaler()\n",
    "    df_prep[scaled_cols] = scaler.fit_transform(df_prep[scaled_cols])\n",
    "\n",
    "# --- 5) Final missingness check with helpful debug output ---\n",
    "total_missing = int(df_prep.isnull().sum().sum())\n",
    "print(f'ðŸ”Ž Total missing values after all preprocessing steps: {total_missing}')\n",
    "if total_missing == 0:\n",
    "    print(\"âœ… No missing values remain after preprocessing.\")\n",
    "else:\n",
    "    print(\"â— Preprocessing left missing values. Breakdown per column (top 50):\")\n",
    "    display(df_prep.isnull().sum().sort_values(ascending=False).head(50))\n",
    "    raise AssertionError(\"âš ï¸ Missing values remain after preprocessing! Inspect above.\")\n",
    "\n",
    "print(f'ðŸ“¦ Prepared dataset shape: {df_prep.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 4) Train/test split, modelling pipeline and evaluation\n",
    "# --------------------------\n",
    "FEATURE_COLS = [c for c in df_prep.columns if c != target_col]\n",
    "X = df_prep[FEATURE_COLS]\n",
    "y = df_prep[target_col].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
    "print('Train shape:', X_train.shape, 'Test shape:', X_test.shape)\n",
    "\n",
    "# Build a safe pipeline (redundant imputer included to protect vs accidental NaNs)\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(max_iter=500))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f'ROC AUC on test set: {auc:.4f}')\n",
    "print('\\nClassification report:')\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print('\\nConfusion matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 5) Save artefacts (model) and provide next steps\n",
    "# --------------------------\n",
    "ARTIFACT_DIR = 'artifacts/models'\n",
    "Path(ARTIFACT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "MODEL_PATH = os.path.join(ARTIFACT_DIR, 'ttc_pd_logreg.joblib')\n",
    "joblib.dump(pipeline, MODEL_PATH)\n",
    "print(f'Model pipeline saved to: {MODEL_PATH}')\n",
    "\n",
    "print('\\nNext steps (suggested):')\n",
    "print('- Expand EDA: IV/WOE, calibration plots, population stability (PSI), and vintage analysis.')\n",
    "print('- Replace logistic with survival or hierarchical models if you have borrower-time series.')\n",
    "print('- Implement cyclical adjustment: e.g., compute long-run average PD by grade and align intercepts.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
